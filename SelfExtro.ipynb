{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24825cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from sklearn import decomposition\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from numpy.linalg import norm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import os\n",
    "\n",
    "### Hyperparameter\n",
    "Step = 1\n",
    "WordDim = 300\n",
    "NormRead = False\n",
    "nNorm = 2\n",
    "\n",
    "GPUIdx = \"0\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=GPUIdx\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "#####\n",
    "# WordVecKinds = \"Extro\" # \"GloVe\", \"TestGloVe\",\"Word2Vec\", or \"FastText\" + \"Extro\"\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f2566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_word_vecs(filename):\n",
    "    print(\"Vectors read from\", filename)\n",
    "    wordVectors = {}\n",
    "    fileObject = open(filename, 'r')\n",
    "    fileObject.readline() # For handling First Line\n",
    "    for line_num, line in enumerate(fileObject):\n",
    "        line = line.strip().lower()\n",
    "        word = line.split()[0]\n",
    "        wordVectors[word] = np.zeros(len(line.split())-1, dtype=np.float64)\n",
    "        vector = line.split()[1:]\n",
    "        if len(vector) == WordDim:\n",
    "            for index, vecVal in enumerate(vector):\n",
    "                wordVectors[word][index] = float(vecVal)\n",
    "            if NormRead:\n",
    "                wordVectors[word] = wordVectors[word] / math.sqrt((wordVectors[word]**2).sum() + 1e-5)\n",
    "        else:\n",
    "            print(line)\n",
    "            break\n",
    "#         if line_num == 1000:\n",
    "#             break\n",
    "    print(\"Done . \")\n",
    "    return wordVectors\n",
    "\n",
    "isNumber = re.compile(r'\\d+')\n",
    "def norm_word(word): # Could Add Preprocessing\n",
    "    if isNumber.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()\n",
    "    \n",
    "def wordVecsLDA(wordVecs):\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    wordVec_np = []\n",
    "    \n",
    "    pbar = tqdm(total = len(newWordVecs))\n",
    "    for v in newWordVecs.values():\n",
    "        pbar.update(1)\n",
    "        wordVec_np.append(v)\n",
    "    wordVec_np = np.array(wordVec_np)\n",
    "    pbar.close()\n",
    "    print(\"Run LDA ...\")\n",
    "    \n",
    "    pbar = tqdm(total = 1)\n",
    "    lda = LinearDiscriminantAnalysis(n_components=WordDim)\n",
    "    print(wordVec_np[:,:-1].shape, wordVec_np[:,-1].shape)\n",
    "    wordVec_np = lda.fit_transform(wordVec_np[:,:-1], wordVec_np[:,-1].astype('int'))\n",
    "    print(wordVec_np.shape)\n",
    "    pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    print(\"LDA Done ...\")\n",
    "    pbar = tqdm(total = len(newWordVecs))\n",
    "    for i, k in enumerate(newWordVecs.keys()):\n",
    "        pbar.update(1)\n",
    "        newWordVecs[k] = wordVec_np[i]\n",
    "    pbar.close()\n",
    "    return newWordVecs\n",
    "\n",
    "def self_extrofit(wordVecs, threshold, it, k):\n",
    "    newWordVecs = deepcopy(wordVecs)\n",
    "    Vocab = newWordVecs.keys()\n",
    "    WVs = torch.tensor([newWordVecs[v] for v in Vocab]).to(device)\n",
    "    print(\"svd\")\n",
    "    U, S, Vh = torch.linalg.svd(WVs)\n",
    "    print(\"done\")    \n",
    "    U_k = U[:,:k]\n",
    "    S_k = S[:k]\n",
    "    WVs = U_k * S_k\n",
    "    labels = torch.zeros(len(Vocab))-1\n",
    "#     labels = labels\n",
    "    cos = torch.nn.CosineSimilarity(dim=1)\n",
    "    ii = 0\n",
    "    pbar = tqdm(total = len(Vocab))\n",
    "    for i, v in enumerate(Vocab):\n",
    "        if labels[i] == -1:\n",
    "            pivot = WVs[i:i+1]#.transpose(0,1)\n",
    "            full_matrix = WVs[i:]\n",
    "#             print(torch.matmul(full_matrix,pivot).shape)\n",
    "            cosine = cos(pivot, full_matrix) # /(torch.norm(full_matrix, dim=1)*torch.norm(pivot))\n",
    "            pick = cosine > threshold\n",
    "            similar_word_idx = torch.nonzero(pick, as_tuple=True)[0] + i # Offset\n",
    "            labels[similar_word_idx] = ii\n",
    "            ii += 1\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    \n",
    "    if it == 0:\n",
    "        print(len(Vocab), \"words will be extrofitted\")\n",
    "        \n",
    "    for i, w in enumerate(Vocab):\n",
    "        newWordVecs[w] = np.hstack((newWordVecs[w], np.mean(wordVecs[w])))\n",
    "        newWordVecs[w] = np.hstack((newWordVecs[w], labels[i]))\n",
    "        \n",
    "#     for word in wvVocab:\n",
    "#         wordidx = wordidx+1\n",
    "#         try:\n",
    "#             wordNeighbours = set(lexicon[word]).intersection(wvVocab)\n",
    "#             numNeighbours = len(wordNeighbours)\n",
    "#         except KeyError: numNeighbours = 0\n",
    "\n",
    "#         if numNeighbours == 0:\n",
    "#             newWordVecs[word][-1] = wordidx\n",
    "#         else:\n",
    "#             newWordVecs[word][-2] += np.mean([np.mean(wordVecs[w]) for w in wordNeighbours])\n",
    "#             for w in wordNeighbours:\n",
    "#                 newWordVecs[w][-1] = wordidx\n",
    "\n",
    "#     ### LDA for dimension reduction\n",
    "    print(\"Dimension Reduction ... \")\n",
    "    newWordVecs = wordVecsLDA(newWordVecs)\n",
    "    return newWordVecs\n",
    "\n",
    "def print_word_vecs(wordVectors, outFileName):\n",
    "    print('Writing down the vectors in', outFileName)\n",
    "    outFile = open(outFileName, 'w')\n",
    "    outFile.write(str(len(wordVectors)) + ' ' + str(WordDim) + '\\n')\n",
    "    pbar = tqdm(total = len(wordVectors), desc = 'Writing')\n",
    "    for word, values in wordVectors.items():\n",
    "        pbar.update(1)\n",
    "        outFile.write(word+' ')\n",
    "        for val in wordVectors[word]:\n",
    "            outFile.write('%.5f' %(val)+' ')\n",
    "        outFile.write('\\n')\n",
    "    outFile.close()\n",
    "    pbar.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c754c7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordVecs = read_word_vecs(\"/mnt/hdd-nfs/Data/PretrainedWV/wiki-news-300d-1M-subword.vec\")\n",
    "wordVecs_extro = self_extrofit(wordVecs, threshold=0.95, it=0, k=300)\n",
    "print(\"Ready (>_<)\")\n",
    "\n",
    "print_word_vecs(wordVecs_extro, \"/mnt/hdd-nfs/temp/fastTextSelfExtro_threshold95_dim300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b7bcdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
